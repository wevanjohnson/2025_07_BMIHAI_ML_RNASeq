---
title: "Regularization Examples"
author: | 
  | W. Evan Johnson, Ph.D.
  | Professor, Division of Infectious Disease
  | Director, Center for Data Science
  | Co-Direcor, Center for Biomedical Informatics and Health AI
  | Rutgers University -- New Jersey Medical School
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    theme: "flatly"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages({
    library(glmnet)
    library(tidyverse)
    library(DT)
})
```

# Regularization Data Examples {.tabset}

## Salary dataset {.tabset}
A researcher wants to determine how employee salaries at a certain company are related to the length of employment, previous experience, and education. The researcher selects eight employees from the company and obtains the data shown below. 

```{r salary}
salary <- tibble(
    Salary = c(57310.00, 57380.00, 54135.00, 56985.00, 58715.00, 60620.00, 59200.00, 60320.00),
    Employment = c(10,5,3,6,8,20,8,14),
    Experience = c(2,6,1,5,8,0,4,6),
    Education = c(16,16,12,14,16,12,18,17)
)
                            
datatable(salary)
```


### LASSO {.tabset}
```{r}
fit <- glmnet(salary[,-1], salary$Salary)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 1000$:
```{r}
coef(fit, s = 1000)
```

##### Results $\lambda = 800$:
```{r}
coef(fit, s = 800)
```

##### Results $\lambda=500$
```{r}
coef(fit, s = 500)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```

### Ridge Regression {.tabset}
```{r}
fit <- glmnet(salary[,-1], salary$Salary, alpha=0)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 1000$:
```{r}
coef(fit, s = 1000)
```

##### Results $\lambda = 800$:
```{r}
coef(fit, s = 800)
```

##### Results $\lambda=500$
```{r}
coef(fit, s = 500)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```

### Elastic Net {.tabset}
```{r}
fit <- glmnet(salary[,-1], salary$Salary, alpha=0.5)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 2000$:
```{r}
coef(fit, s = 2000)
```

##### Results $\lambda = 1000$:
```{r}
coef(fit, s = 1000)
```

##### Results $\lambda=500$
```{r}
coef(fit, s = 500)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```



## Car sales dataset {.tabset}

```{r}
cars <- read_csv("Car price prediction.csv")
datatable(cars)
```


### LASSO {.tabset}
```{r}
fit <- glmnet(cars[,c(2,4:8)], cars$selling_price)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 200,000$:
```{r}
coef(fit, s = 200000)
```

##### Results $\lambda = 100,000$:
```{r}
coef(fit, s = 100000)
```

##### Results $\lambda=20,000$
```{r}
coef(fit, s = 20000)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```

#### Cross-validation {.tabset}
```{r}
set.seed(0)
design <- cars %>% model.matrix(~fuel+seller_type+transmission+owner, data = .)
design <- cbind(year = cars$year, km_driven = cars$km_driven,design)
cvfit <- cv.glmnet(design, cars$selling_price)
```

##### Cross-validation plot
```{r}
plot(cvfit)
```

##### $\lambda_{min}$ value
We can get the value of $\lambda_{min}$ and the model coefficients:
```{r }
cvfit$lambda.min
```

##### Coefficients at $\lambda_{min}$
```{r}
coef(cvfit, s = "lambda.min")
```

### Ridge Regression {.tabset}
```{r}
fit <- glmnet(salary[,-1], salary$Salary, alpha=0)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 200,000$:
```{r}
coef(fit, s = 200000)
```

##### Results $\lambda = 100,000$:
```{r}
coef(fit, s = 100000)
```

##### Results $\lambda=20,000$
```{r}
coef(fit, s = 20000)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```

### Elastic Net {.tabset}
```{r}
fit <- glmnet(salary[,-1], salary$Salary, alpha=0.5)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 200,000$:
```{r}
coef(fit, s = 200000)
```

##### Results $\lambda = 100,000$:
```{r}
coef(fit, s = 100000)
```

##### Results $\lambda=20,000$
```{r}
coef(fit, s = 20000)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```

#### Cross-validation {.tabset}
```{r}
set.seed(0)
design <- cars %>% model.matrix(~fuel+seller_type+transmission+owner, data = .)
design <- cbind(year = cars$year, km_driven = cars$km_driven,design)
cvfit <- cv.glmnet(design, cars$selling_price)
```

##### Cross-validation plot
```{r}
plot(cvfit)
```

##### $\lambda_{min}$ value
We can get the value of $\lambda_{min}$ and the model coefficients:
```{r }
cvfit$lambda.min
```

##### Coefficients at $\lambda_{min}$
```{r}
coef(cvfit, s = "lambda.min")
```


## Cereal dataset {.tabset}
Eat too much sugary cereal? Ruin your appetite with this dataset! If you like to eat cereal, do yourself a favor and avoid this dataset at all costs. After seeing these data it will never be the same for me to eat Fruity Pebbles again.
```{r}
cereal <- read_csv("cereal.csv")
datatable(cereal)
```

```{r}
fit <- glmnet(cereal[,c(2:15)], cereal$rating)
```

#### Visualize L1
We can visualize the coefficients by executing the plot method:
```{r}
plot(fit)
```

#### Print summary
A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
```{r}
print(fit)
```

#### Results for various $\lambda$s {.tabset}
##### Results $\lambda = 8$:
```{r}
coef(fit, s = 8)
```

##### Results $\lambda = 5$:
```{r}
coef(fit, s = 5)
```

##### Results $\lambda=3$
```{r}
coef(fit, s = 3)
```

##### Results $\lambda=1$
```{r}
coef(fit, s = 1)
```

